{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDAPfavqYLa6"
      },
      "outputs": [],
      "source": [
        "!pip install ucimlrepo\n",
        "!pip install pandas\n",
        "!pip install scipy\n",
        "!pip install tensorflow==2.10.0\n",
        "!pip install keras==2.10.0\n",
        "!pip install kormos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmpGcuAZLkm4"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# This will prompt you to upload a file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX0yQgGDwV-M"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import kagglehub\n",
        "import random\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvYOhjn7LdM8"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv('heart_disease_uci.csv')\n",
        "df1 = pd.read_csv('bank.csv')\n",
        "df2 = pd.read_csv('winequality-red.csv')\n",
        "\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Access data\n",
        "##############################################################################################################################\n",
        "\n",
        "# Heart Features\n",
        "# Heart Target\n",
        "\n",
        "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_columns)\n",
        "#col_means = {col:  np.mean(df_encoded[col]) for col in df_encoded.columns}\n",
        "#df_encoded = df_encoded.fillna(col_means)\n",
        "X_heart = df_encoded.drop(columns=['num'])\n",
        "y_heart = df_encoded['num']\n",
        "\n",
        "# Bank Features\n",
        "categorical_columns = df1.select_dtypes(include=['object', 'category']).columns\n",
        "df1_encoded = pd.get_dummies(df1, columns=categorical_columns)\n",
        "#col_means = {col:  np.mean(df_encoded[col]) for col in df1_encoded.columns}\n",
        "#df1_encoded = df1_encoded.fillna(col_means)\n",
        "X_bank = df1_encoded.drop(columns=['deposit_no', 'deposit_yes'])\n",
        "# Bank Target\n",
        "y_bank = df1_encoded['deposit_yes']\n",
        "\n",
        "# Ban Features\n",
        "categorical_columns = df2.select_dtypes(include=['object', 'category']).columns\n",
        "df2_encoded = pd.get_dummies(df2, columns=categorical_columns)\n",
        "#col_means = {col:  np.mean(df2_encoded[col]) for col in df2_encoded.columns}\n",
        "#df2_encoded = df2_encoded.fillna(col_means)\n",
        "X_wine = df2_encoded.drop(columns=['quality'])\n",
        "# Bank Target\n",
        "y_wine = df2_encoded['quality']\n",
        "\n",
        "##############################################################################################################################\n",
        "\n",
        "#Print to check\n",
        "##############################################################################################################################\n",
        "\n",
        "print(\"Heart Disease Features:\\n\", X_heart)\n",
        "\n",
        "print(\"Heart Disease Target:\\n\", y_heart)\n",
        "\n",
        "print(\"\\n####################################################################################\\n\")\n",
        "print(\"Bank Marketing Features:\\n\", X_bank)\n",
        "\n",
        "print(\"Bank Marketing Target:\\n\", y_bank)\n",
        "print(\"\\n####################################################################################\\n\")\n",
        "print(\"Wine Quality:\\n\", X_wine)\n",
        "\n",
        "print(\"Wine Quality Target:\\n\", y_wine)\n",
        "print(\"\\n\")\n",
        "\n",
        "##############################################################################################################################\n",
        "\n",
        "# Data Split\n",
        "##############################################################################################################################\n",
        "# Train/Test\n",
        "X_Htrain, X_Htest, y_Htrain, y_Htest = train_test_split(X_heart, y_heart, test_size=0.2, random_state=seed)\n",
        "X_Wtrain, X_Wtest, y_Wtrain, y_Wtest = train_test_split(X_wine, y_wine, test_size=0.2, random_state=seed)\n",
        "X_Btrain, X_Btest, y_Btrain, y_Btest = train_test_split(X_bank, y_bank, test_size=0.2, random_state=seed)\n",
        "\n",
        "# Train/Val\n",
        "X_Htrain, X_Hval, y_Htrain, y_Hval = train_test_split(X_Htrain, y_Htrain, test_size=0.3, random_state=seed)\n",
        "X_Wtrain, X_Wval, y_Wtrain, y_Wval = train_test_split(X_Wtrain, y_Wtrain, test_size=0.3, random_state=seed)\n",
        "X_Btrain, X_Bval, y_Btrain, y_Bval = train_test_split(X_Btrain, y_Btrain, test_size=0.3, random_state=seed)\n",
        "\n",
        "###############################################################################################################################\n",
        "\n",
        "# Print to check\n",
        "###############################################################################################################################\n",
        "print(\"Heart Disease Dataset:\")\n",
        "print(\"Training features shape:\", X_Htrain.shape)\n",
        "print(\"Validation features shape:\", X_Hval.shape)\n",
        "print(\"Testing features shape:\", X_Htest.shape)\n",
        "print(\"Training target shape:\", y_Htrain.shape)\n",
        "print(\"Validation target shape:\", y_Hval.shape)\n",
        "print(\"Testing target shape:\", y_Htest.shape)\n",
        "\n",
        "print(\"\\nWine Quality Dataset:\")\n",
        "print(\"Training features shape:\", X_Wtrain.shape)\n",
        "print(\"Validation features shape:\", X_Wval.shape)\n",
        "print(\"Testing features shape:\", X_Wtest.shape)\n",
        "print(\"Training target shape:\", y_Wtrain.shape)\n",
        "print(\"Validation target shape:\", y_Wval.shape)\n",
        "print(\"Testing target shape:\", y_Wtest.shape)\n",
        "\n",
        "print(\"\\nBank Dataset:\")\n",
        "print(\"Training features shape:\", X_Btrain.shape)\n",
        "print(\"Validation features shape:\", X_Bval.shape)\n",
        "print(\"Testing features shape:\", X_Btest.shape)\n",
        "print(\"Training target shape:\", y_Btrain.shape)\n",
        "print(\"Validation target shape:\", y_Bval.shape)\n",
        "print(\"Testing target shape:\", y_Btest.shape)\n",
        "\n",
        "#################################################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g_a0hTg8Sv1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from kormos.models import BatchOptimizedSequentialModel\n",
        "\n",
        "def build_model(optim_type, num_classes, learning_rate_init, batch_size, hidden_layer_sizes, activation):\n",
        "  model = BatchOptimizedSequentialModel()\n",
        "  for layer_size in hidden_layer_sizes:\n",
        "    model.add(keras.layers.Dense(layer_size, activation=activation))\n",
        "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=optim_type, metrics=['accuracy'])\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT9Hg_a7Fa7G"
      },
      "outputs": [],
      "source": [
        "def random_hyperparamaters():\n",
        "  learning_rate = random.choice([0.0001, 0.001, 0.01, 0.1])\n",
        "  batch_size = random.choice([16, 32, 64, 128])\n",
        "  num_layers = random.choice([1, 2, 3, 4])\n",
        "  nodes_per_layer = random.choice([32, 64, 128, 256])\n",
        "  activation_function = random.choice(['relu', 'tanh'])\n",
        "\n",
        "  return {'learning rate': learning_rate, 'batch size' : batch_size, 'num of layers' : num_layers, 'nodes per layer': nodes_per_layer, 'activation function' : activation_function }\n",
        "\n",
        "\n",
        "def generate_sample(n):\n",
        "\n",
        "    sample = set()\n",
        "\n",
        "    while(len(sample) < n):\n",
        "\n",
        "      sample.add(tuple(random_hyperparamaters().items()))\n",
        "\n",
        "    return [dict(comb) for comb in sample]\n",
        "\n",
        "def map_hyperparameters(params):\n",
        "\n",
        "    hidden_layer_sizes = tuple([params['nodes per layer']] * params['num of layers'])\n",
        "    activation = params['activation function']\n",
        "\n",
        "    return {\n",
        "        'learning_rate_init': params['learning rate'],\n",
        "        'batch_size': params['batch size'],\n",
        "        'hidden_layer_sizes': hidden_layer_sizes,\n",
        "        'activation': activation\n",
        "    }\n",
        "\n",
        "# Function to train and evaluate the MLP with the best hyperparameters\n",
        "def train_mlp_with_hyperparameters(best_hyperparams, X_tr, y_tr, X_val, y_val, X_te, y_te, seed):\n",
        "      mlp = MLPClassifier(**best_hyperparams, shuffle=True, random_state=seed, verbose=False)\n",
        "      mlp.fit(X_tr, y_tr)\n",
        "      train_accuracy = mlp.score(X_tr, y_tr)\n",
        "      val_accuracy = mlp.score(X_val, y_val)\n",
        "      test_accuracy = mlp.score(X_te, y_te)\n",
        "\n",
        "      print('Best hyperparameters performance:')\n",
        "      print(f'Training accuracy: {train_accuracy}')\n",
        "      print(f'Validation accuracy: {val_accuracy}')\n",
        "      print(f'Test accuracy: {test_accuracy}')\n",
        "      return train_accuracy, val_accuracy, test_accuracy\n",
        "\n",
        "def run_test(X_tr, y_tr, X_val, y_val, X_te, y_te):\n",
        "\n",
        "  sample = generate_sample(50)\n",
        "\n",
        "\n",
        "  bfgs_results = []\n",
        "  cg_results = []\n",
        "\n",
        "  imputer = SimpleImputer(strategy='mean')\n",
        "  X_tr = imputer.fit_transform(X_tr)\n",
        "  X_val = imputer.transform(X_val)\n",
        "  X_te = imputer.transform(X_te)\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X_tr = scaler.fit_transform(X_tr)\n",
        "  X_val = scaler.transform(X_val)\n",
        "  X_te = scaler.transform(X_te)\n",
        "\n",
        "  bfgs_tot = None\n",
        "  cg_tot = None\n",
        "\n",
        "  for params in sample:\n",
        "      mapped_params = map_hyperparameters(params)\n",
        "      start = datetime.datetime.now()\n",
        "      bfgs = build_model('L-BFGS-B', len(np.unique(y_tr)), **mapped_params)\n",
        "      bfgs.build(X_tr.shape)\n",
        "      bfgs_history = bfgs.fit_batch(X_tr, y_tr, validation_data=(X_val, y_val), epochs=10, steps_per_epoch=50, batch_size = mapped_params['batch_size'])\n",
        "      train_accuracy = np.min(bfgs.history.history['loss'])\n",
        "      val_accuracy = np.max(bfgs.history.history['val_accuracy'])\n",
        "      bfgs_results.append((params, train_accuracy, val_accuracy))\n",
        "      if bfgs_tot is None:\n",
        "        bfgs_tot = (datetime.datetime.now() - start).total_seconds()\n",
        "      else:\n",
        "        bfgs_tot += (datetime.datetime.now() - start).total_seconds()\n",
        "      start = datetime.datetime.now()\n",
        "      cg = build_model('CG', len(np.unique(y_tr)), **mapped_params)\n",
        "      cg.build(X_tr.shape)\n",
        "      cg_history = cg.fit_batch(X_tr, y_tr, validation_data=(X_val, y_val), epochs=10, steps_per_epoch=50, batch_size = mapped_params['batch_size'])\n",
        "      train_accuracy = np.min(cg_history.history['loss'])\n",
        "      val_accuracy = np.max(cg_history.history['val_accuracy'])\n",
        "      cg_results.append((params, train_accuracy, val_accuracy))\n",
        "      if cg_tot is None:\n",
        "        cg_tot = (start - datetime.datetime.now()).total_seconds()\n",
        "      else:\n",
        "        cg_tot += (start - datetime.datetime.now()).total_seconds()\n",
        "\n",
        "  print(\"BFGS Time: \", bfgs_tot/len(bfgs_results))\n",
        "  print(\"CG Time: \", cg_tot/len(cg_results))\n",
        "\n",
        "  # Select the best hyperparameters based on validation accuracy\n",
        "  best_params_bfgs, best_train_accuracy_bfgs, best_val_accuracy_bfgs = max(bfgs_results, key=lambda x: x[2])\n",
        "  best_hyperparams_bfgs = map_hyperparameters(best_params_bfgs)\n",
        "\n",
        "  print(\"Best BFGS hyperparameters:\", best_params_bfgs)\n",
        "\n",
        "  best_params_cg, best_train_accuracy_cg, best_val_accuracy_cg = max(cg_results, key=lambda x: x[2])\n",
        "  best_hyperparams_cg = map_hyperparameters(best_params_cg)\n",
        "\n",
        "  print(\"Best CG hyperparameters:\", best_params_cg)\n",
        "\n",
        "  # Check your results\n",
        "  print('BFGS Results: ')\n",
        "  train_mlp_with_hyperparameters(best_hyperparams_bfgs, X_tr, y_tr, X_val, y_val, X_te, y_te, seed)\n",
        "\n",
        "  print('CG Results: ')\n",
        "  train_mlp_with_hyperparameters(best_hyperparams_cg, X_tr, y_tr, X_val, y_val, X_te, y_te, seed)\n",
        "\n",
        "print('Running for Heart Disease:')\n",
        "run_test(X_Htrain, y_Htrain, X_Hval, y_Hval, X_Htest, y_Htest)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Running for Wine Quality:')\n",
        "run_test(X_Wtrain, y_Wtrain, X_Wval, y_Wval, X_Wtest, y_Wtest)"
      ],
      "metadata": {
        "id": "w4fArXD8kkOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Running for Bank:')\n",
        "run_test(X_Btrain, y_Btrain, X_Bval, y_Bval, X_Btest, y_Btest)"
      ],
      "metadata": {
        "id": "rz0RSkT2ksaD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}